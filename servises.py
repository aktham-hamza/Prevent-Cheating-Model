# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
import cv2                  # Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ
import pickle               # Ù…ÙƒØªØ¨Ø© Ù„Ø­ÙØ¸ ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
import numpy as np          # Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©
import os                   # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
import json                 # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª JSON
import csv                  # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª CSV
from datetime import datetime  # Ù…ÙƒØªØ¨Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„ØªØ§Ø±ÙŠØ®
from sklearn.neighbors import KNeighborsClassifier  # Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ KNN
from imutils import face_utils  # Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¬ÙˆÙ‡
import dlib                 # Ù…ÙƒØªØ¨Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ÙˆØ§Ù„Ù†Ù‚Ø§Ø·
import threading            # Ù…ÙƒØªØ¨Ø© ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
import platform             # Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø±ÙØ© Ù†ÙˆØ¹ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„
import time                 # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø²Ù…Ù†

# ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙƒÙ„Ø§Ø³ FaceDetection Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙˆØ¬ÙˆÙ‡
class FaceDetection:

    # Ø¯Ø§Ù„Ø© Ù„ØªØ´ØºÙŠÙ„ ØµÙˆØª ØªÙ†Ø¨ÙŠÙ‡ÙŠ Ø¹Ù†Ø¯ Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…Ø¹ Ø§Ù„ØµÙˆØ± Ø£Ùˆ Ø¹Ù†Ø¯ Ø§Ù„Ø§ÙƒØªÙ…Ø§Ù„
    def play_sound():
        if platform.system() == "Windows":
            import winsound
            winsound.Beep(1000, 500)  # Ù†ØºÙ…Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù…Ø¯Ø© Ù†ØµÙ Ø«Ø§Ù†ÙŠØ©
        else:
            os.system('play -nq -t alsa synth 0.5 sine 1000')  # Ù„ÙŠÙ†ÙˆÙƒØ³ Ø£Ùˆ Ù…Ø§Ùƒ

    # Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ ØµÙˆØ± Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    def collect_faces(user_id, name, max_faces=20):
        # ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…Ù† Ø§Ù„Ù…ÙˆØ¨Ø§ÙŠÙ„ Ù„Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙŠ Ø§Ù„Ø§Ø¨ØªÙˆØ¨ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø±Ø¨Ø· Ø§Ù„Ø§Ø¨ØªÙˆØ¨ ÙˆØ§Ù„Ù…ÙˆØ¨Ø§ÙŠÙ„ Ø¹Ù„Ù‰ 
        # IP Ø§Ù„Ø´Ø¨ÙƒØ© Ø°Ø§ØªÙ‡Ø§
        phone_IP = "http://192.168.217.136:8080/video"
        video=cv2.VideoCapture(phone_IP)
        
        # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙ†Ù ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ø¬Ø§Ù‡Ø² Ù…Ù† OpenCV
        facedetect = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
        
        if facedetect.empty():
            raise Exception("ğŸš¨ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ØµÙ†Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡.")

        faces_data = []  # ØªØ®Ø²ÙŠÙ† Ø§Ù„ØµÙˆØ±
        frame_count = 0  # Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        completed = False  # Ø­Ø§Ù„Ø© Ø§ÙƒØªÙ…Ø§Ù„ Ø¬Ù…Ø¹ Ø§Ù„ØµÙˆØ±

        while True:
            ret, frame = video.read()
            if not ret:
                break
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = facedetect.detectMultiScale(gray, 1.3, 5)

            for (x, y, w, h) in faces:
                crop_img = frame[y:y+h, x:x+w]
                resized_img = cv2.resize(crop_img, (50, 50))

                if len(faces_data) < max_faces and frame_count % 10 == 0:
                    faces_data.append(resized_img)

                frame_count += 1

                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ© Ù„Ù„ØªÙ‚Ø¯Ù…
                percent = int((len(faces_data) / max_faces) * 100)

                # Ø±Ø³Ù… Ø´Ø±ÙŠØ· Ø¨ÙŠØ¶Ø§ÙˆÙŠ Ø­ÙˆÙ„ Ø§Ù„ÙˆØ¬Ù‡ Ù…Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„Ù†Ø³Ø¨Ø©
                center_x = x + w // 2
                center_y = y + h // 2
                axes_length = (w // 2, int(h * 0.6))
                cv2.ellipse(frame, (center_x, center_y), axes_length, 0, 0, 360, (80, 80, 80), 2)
                angle = int((percent / 100) * 360)
                green_color = (0, 255, 0)
                base_thickness = 8
                cv2.ellipse(frame, (center_x, center_y), axes_length, 0, 0, angle, green_color, base_thickness)

                cv2.putText(frame, f"{percent}%", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, green_color, 2)

                if percent >= 100 and not completed:
                    threading.Thread(target=FaceDetection.play_sound).start()
                    completed = True

            # Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© Ø¹Ù†Ø¯ Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…Ø¹ Ø§Ù„ØµÙˆØ±
            if completed:
                cv2.putText(frame, "Face collection complete!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)

            cv2.imshow("Capturing Faces", frame)
            if cv2.waitKey(1) == ord('q') or len(faces_data) >= max_faces:
                break

        # Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        video.release()
        cv2.destroyAllWindows()

        # ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù„Ù„Ø­ÙØ¸
        faces_data = np.asarray(faces_data)
        faces_data = faces_data.reshape(len(faces_data), -1)

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ "models" Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯
        os.makedirs("models", exist_ok=True)

        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
        names_path = 'models/names.pkl'
        faces_path = 'models/faces_data.pkl'
        users_json_path = 'models/users.json'

        # ØªØ®Ø²ÙŠÙ† Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        if os.path.exists(names_path):
            with open(names_path, 'rb') as f:
                user_ids = pickle.load(f)
        else:
            user_ids = []

        user_ids += [user_id] * len(faces_data)
        with open(names_path, 'wb') as f:
            pickle.dump(user_ids, f)

        # ØªØ®Ø²ÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±
        if os.path.exists(faces_path):
            with open(faces_path, 'rb') as f:
                faces = pickle.load(f)
            faces = np.append(faces, faces_data, axis=0)
        else:
            faces = faces_data

        with open(faces_path, 'wb') as f:
            pickle.dump(faces, f)

        # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        if os.path.exists(users_json_path):
            with open(users_json_path, 'r') as f:
                users = json.load(f)
        else:
            users = {}

        users[str(user_id)] = name

        with open(users_json_path, 'w') as f:
            json.dump(users, f, indent=4)

        # Ø¥Ø±Ø¬Ø§Ø¹ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ø¬Ø§Ø­
        return {
            "status": "success",
            "user_id": user_id,
            "name": name,
            "message": f"âœ…  collected {len(faces_data)}  photo of face '{name}' ID {user_id}"
        }

    # Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬Ù‡ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¶ÙˆØ±
    def recognize_and_mark_attendance():
        # ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙˆØªØ­Ù…ÙŠÙ„ Ù…ØµÙ†Ù ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡
        phone_IP = "http://192.168.217.136:8080/video"
        video=cv2.VideoCapture(phone_IP)
        facedetect = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        with open('models/names.pkl', 'rb') as w:
            LABELS = pickle.load(w)
        with open('models/faces_data.pkl', 'rb') as f:
            FACES = pickle.load(f)

        # ØªØ¯Ø±ÙŠØ¨ Ù…ØµÙ†Ù KNN
        knn = KNeighborsClassifier(n_neighbors=5)
        knn.fit(FACES, LABELS)

        # ØªØ­Ù…ÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        users_json_path = 'models/users.json'
        if os.path.exists(users_json_path):
            with open(users_json_path, 'r') as f:
                user_names = json.load(f)
        else:
            user_names = {}

        distance_threshold = 2600.0  # Ø­Ø¯ Ø§Ù„Ù…Ø³Ø§ÙØ© Ù„ØªØµÙ†ÙŠÙ ØµØ­ÙŠØ­
        date = datetime.now().strftime("%d-%m-%Y")
        filename = f"models/Attendance_{date}.csv"
        os.makedirs("models", exist_ok=True)

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø­Ø¶ÙˆØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not os.path.exists(filename):
            with open(filename, "w", newline='') as f:
                csv.writer(f).writerow(["NAME", "TIME"])

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³Ø¬Ù„Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹
        logged_names = set()
        with open(filename, "r") as f:
            reader = csv.reader(f)
            next(reader, None)
            for row in reader:
                if row:
                    logged_names.add(row[0])

        recognized_count = {}
        target_count = 5

        # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù ÙˆØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        while True:
            ret, frame = video.read()
            if not ret:
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = facedetect.detectMultiScale(gray, 1.2, 5)

            for (x, y, w, h) in faces:
                crop_img = frame[y:y+h, x:x+w, :]
                resized_img = cv2.resize(crop_img, (50, 50)).flatten().reshape(1, -1)

                distances, _ = knn.kneighbors(resized_img, n_neighbors=1)
                mean_distance = distances[0][0]

                if mean_distance < distance_threshold:
                    output = knn.predict(resized_img)
                    user_id = str(output[0])
                    name_text = user_names.get(user_id, f"ID_{user_id}")

                    recognized_count[user_id] = recognized_count.get(user_id, 0) + 1

                    if name_text not in logged_names:
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        with open(filename, "a", newline='') as f:
                            csv.writer(f).writerow([name_text, timestamp])
                        logged_names.add(name_text)

                    color = (0, 255, 0)
                else:
                    user_id = None
                    name_text = "Unknown"
                    color = (0, 0, 255)

                # Ø±Ø³Ù… Ø§Ù„Ù…Ø³ØªØ·ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù…ÙƒØªØ´Ù
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                cv2.putText(frame, name_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

            cv2.imshow("Face Recognition", frame)
            if cv2.waitKey(1) == ord('q'):
                break

            # Ø¥Ø°Ø§ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù
            for user_id, count in recognized_count.items():
                if count >= target_count:
                    video.release()
                    cv2.destroyAllWindows()
                    return {
                        "status": "success",
                        "recognized_id": user_id,
                        "name": user_names.get(user_id, "Unknown"),
                        "count": count,
                        "date": date,
                        "csv": filename,
                        "message": f"{user_names.get(user_id, 'Unknown')} ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡ {count} Ù…Ø±Ø§Øª."
                    }

        video.release()
        cv2.destroyAllWindows()
        return {
            "status": "failed",
            "recognized": None,
            "message": "Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£ÙŠ ÙˆØ¬Ù‡ Ø¨Ù…Ø§ ÙÙŠÙ‡ Ø§Ù„ÙƒÙØ§ÙŠØ©."
        }

    # Ø¯Ø§Ù„Ø© Ù„Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø®Ø²Ù†Ø©
    def clear_training_data():
        files_to_clear = [
            'models/names.pkl',
            'models/faces_data.pkl',
            'models/users.json'
        ]

        for file_path in files_to_clear:
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"ğŸ§¹ ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù: {file_path}")
            else:
                print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")

    # Ø¯Ø§Ù„Ø© Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù†Ø¸Ø± ÙˆØªØ­Ø°ÙŠØ± Ù…Ù† Ø§Ù„ØºØ´ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†
    def face_traking(is_sheet, exam_time):
        
        # Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        detector = dlib.get_frontal_face_detector()
        model_path = os.path.join(os.path.dirname(__file__), "models/shape_predictor_68_face_landmarks.dat")
        predictor = dlib.shape_predictor(model_path)
        phone_IP = "http://192.168.217.136:8080/video"
        video=cv2.VideoCapture(phone_IP)

        total_frames = 0
        cheat_frames = 0
        start_time = time.time()

        # Ø¯Ø§Ù„Ø© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¹ÙŠÙ†
        def get_eye_position(eye_points, shape, gray):
            eye_region = np.array([shape[i] for i in eye_points], np.int32)
            min_x, max_x = np.min(eye_region[:, 0]), np.max(eye_region[:, 0])
            min_y, max_y = np.min(eye_region[:, 1]), np.max(eye_region[:, 1])
            eye_frame = gray[min_y:max_y, min_x:max_x]
            eye_frame = cv2.resize(eye_frame, (80, 30))
            _, threshold_eye = cv2.threshold(eye_frame, 70, 255, cv2.THRESH_BINARY_INV)
            height, width = threshold_eye.shape
            left_part = threshold_eye[:, 0:width//2]
            right_part = threshold_eye[:, width//2:]

            left_intensity = np.sum(left_part)
            right_intensity = np.sum(right_part)

            if left_intensity > right_intensity:
                return "RIGHT"
            elif right_intensity > left_intensity:
                return "LEFT"
            else:
                return "CENTER"

        while True:
            ret, image = video.read()
            if not ret:
                break

            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            rects = detector(gray, 0)

            for rect in rects:
                shape = predictor(gray, rect)
                shape = face_utils.shape_to_np(shape)

                left_eye_pos = get_eye_position([36, 37, 38, 39, 40, 41], shape, gray)
                right_eye_pos = get_eye_position([42, 43, 44, 45, 46, 47], shape, gray)

                # ØªØ­Ø¯ÙŠØ¯ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù†Ø¸Ø±Ø©
                if left_eye_pos == "LEFT" and right_eye_pos == "LEFT":
                    gaze_direction = "Looking LEFT"
                elif left_eye_pos == "RIGHT" and right_eye_pos == "RIGHT":
                    gaze_direction = "Looking RIGHT"
                else:
                    gaze_direction = "Looking CENTER"

                cv2.putText(image, gaze_direction, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

                for (x, y) in shape:
                    cv2.circle(image, (x, y), 2, (0, 255, 0), -1)

                total_frames += 1
                if gaze_direction in ["Looking LEFT", "Looking RIGHT"]:
                    cheat_frames += 1

            elapsed_time = time.time() - start_time
            if elapsed_time >= exam_time:
                break

            cv2.imshow("Gaze Detection", image)
            if cv2.waitKey(1) == ord('q'):
                break

        video.release()
        cv2.destroyAllWindows()

        if total_frames == 0:
            return {"status": "error", "message": "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ÙˆØ¬Ù‡"}

        cheat_percentage = (cheat_frames / total_frames) * 100

        if cheat_percentage > 10:
            if is_sheet:
                return {"status": "sheet detect"}

            return {"status": "cheat warning"}
        else:
            return {"status": "normal"}
